{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mamidi2001/deeplearning_project/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### End-to-End Keras implementation on top of Tensorflow 2.3"
      ],
      "metadata": {
        "id": "8DLrWEaDti5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "5vqo0Qtkti5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###====================== HYPER-PARAMETERS ===========================###\n",
        "## Adam\n",
        "batch_size = 5  \n",
        "lr_v = 2e-4\n",
        "tot_sample= 100  # Totall traning images\n",
        "## adversarial learning (SRGAN)\n",
        "n_epoch = 500\n",
        "## initialize G\n",
        "n_epoch_init = n_epoch//10\n",
        "\n",
        "# create folders to save result images and trained models\n",
        "save_dir = \"samples\"\n",
        "checkpoint_dir = \"models\"\n",
        "#track image as per index\n",
        "save_ind= 16\n",
        "\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)"
      ],
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "4wk5hG-Zti5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load(path,shape):\n",
        "    img= cv2.imread(path)\n",
        "    img= cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img= cv2.resize(img, shape)\n",
        "    return img\n",
        "\n",
        "\n",
        "def get_data(path):\n",
        "    X=[]\n",
        "    Y=[]\n",
        "    for folder in glob.glob(path+ str('/*')):\n",
        "        for img_path in glob.glob(folder+ str('/*')):      \n",
        "            if folder == os.path.join(path, 'HR'):\n",
        "                X.append(load(img_path, (384, 384)))\n",
        "            elif folder == os.path.join(path, 'LR'):\n",
        "                Y.append(load(img_path, (96,96)))\n",
        "\n",
        "    X= np.array(X)\n",
        "    Y= np.array(Y)\n",
        "    return X/255.0, Y/255.0\n",
        "    "
      ],
      "metadata": {
        "trusted": true,
        "id": "wzGSbuu6ti5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HR_train, LR_train= get_data('../input/super-image-resolution/Data')\n",
        "HR_train.shape, LR_train.shape"
      ],
      "metadata": {
        "trusted": true,
        "id": "Re_IcdVDti5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(HR_train[65])"
      ],
      "metadata": {
        "trusted": true,
        "id": "w9LwxMnGti5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f, ax= plt.subplots(1,2, figsize=(16, 6))\n",
        "ax[0].imshow(LR_train[save_ind], aspect='auto')\n",
        "ax[1].imshow(HR_train[save_ind], aspect='auto')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "dadBS8R-ti5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GAN-Model\n",
        "\n",
        "### 1. Pre-Functions"
      ],
      "metadata": {
        "id": "a_OtzOgEti5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, add,\\\n",
        "                                    BatchNormalization, Activation, LeakyReLU, Layer\n",
        "\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "trusted": true,
        "id": "TYHVO2oiti5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SubpixelConv2D(Layer):\n",
        "    \"\"\" Subpixel Conv2D Layer\n",
        "    upsampling a layer from (h, w, c) to (h*r, w*r, c/(r*r)),\n",
        "    where r is the scaling factor, default to 4\n",
        "    # Arguments\n",
        "    upsampling_factor: the scaling factor\n",
        "    # Input shape\n",
        "        Arbitrary. Use the keyword argument `input_shape`\n",
        "        (tuple of integers, does not include the samples axis)\n",
        "        when using this layer as the first layer in a model.\n",
        "    # Output shape\n",
        "        the second and the third dimension increased by a factor of\n",
        "        `upsampling_factor`; the last layer decreased by a factor of\n",
        "        `upsampling_factor^2`.\n",
        "    # References\n",
        "        Real-Time Single Image and Video Super-Resolution Using an Efficient\n",
        "        Sub-Pixel Convolutional Neural Network Shi et Al. https://arxiv.org/abs/1609.05158\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, upsampling_factor=2, **kwargs):\n",
        "        super(SubpixelConv2D, self).__init__(**kwargs)\n",
        "        self.upsampling_factor = upsampling_factor\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        last_dim = input_shape[-1]\n",
        "        factor = self.upsampling_factor * self.upsampling_factor\n",
        "        if last_dim % (factor) != 0:\n",
        "            raise ValueError('Channel ' + str(last_dim) + ' should be of '\n",
        "                             'integer times of upsampling_factor^2: ' +\n",
        "                             str(factor) + '.')\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        return tf.nn.depth_to_space( inputs, self.upsampling_factor )\n",
        "\n",
        "    def get_config(self):\n",
        "        config = { 'upsampling_factor': self.upsampling_factor, }\n",
        "        base_config = super(SubpixelConv2D, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        factor = self.upsampling_factor * self.upsampling_factor\n",
        "        input_shape_1 = None\n",
        "        if input_shape[1] is not None:\n",
        "            input_shape_1 = input_shape[1] * self.upsampling_factor\n",
        "        input_shape_2 = None\n",
        "        if input_shape[2] is not None:\n",
        "            input_shape_2 = input_shape[2] * self.upsampling_factor\n",
        "        dims = [ input_shape[0],\n",
        "                 input_shape_1,\n",
        "                 input_shape_2,\n",
        "                 int(input_shape[3]/factor)\n",
        "               ]\n",
        "        return tuple( dims )"
      ],
      "metadata": {
        "trusted": true,
        "id": "kkppYarvti5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generator\n",
        "def get_G(input_shape):\n",
        "    # w_init = tf.random_normal_initializer(stddev=0.02)\n",
        "    g_init = tf.random_normal_initializer(1., 0.02)\n",
        "    relu= Activation('relu')\n",
        "\n",
        "    nin= Input(shape= input_shape)\n",
        "    n= Conv2D(64, (3,3), padding='SAME', activation= 'relu',\n",
        "                        kernel_initializer='HeNormal')(nin)\n",
        "    temp= n\n",
        "\n",
        "\n",
        "    # B residual blocks\n",
        "    for i in range(3):\n",
        "        nn= Conv2D(64, (3,3), padding='SAME', kernel_initializer='HeNormal')(n)\n",
        "        nn= BatchNormalization(gamma_initializer= g_init)(nn)\n",
        "        nn= relu(nn)\n",
        "        nn= Conv2D(64, (3,3), padding='SAME', kernel_initializer='HeNormal')(n)\n",
        "        nn= BatchNormalization(gamma_initializer= g_init)(nn)\n",
        "\n",
        "        nn= add([n, nn])\n",
        "        n= nn\n",
        "\n",
        "    n= Conv2D(64, (3,3), padding='SAME', kernel_initializer='HeNormal')(n)\n",
        "    n= BatchNormalization(gamma_initializer= g_init)(n)\n",
        "    n= add([n, temp])\n",
        "    # B residual blacks end\n",
        "\n",
        "    n= Conv2D(256, (3,3), padding='SAME', kernel_initializer='HeNormal')(n)\n",
        "    n= SubpixelConv2D(upsampling_factor=2)(n)\n",
        "    n= relu(n)\n",
        "\n",
        "    n= Conv2D(256, (3,3), padding='SAME', kernel_initializer='HeNormal')(n)\n",
        "    n= SubpixelConv2D(upsampling_factor=2)(n)\n",
        "    n= relu(n)\n",
        "\n",
        "    nn= Conv2D(3, (1,1), padding='SAME', kernel_initializer='HeNormal', activation= 'tanh')(n)\n",
        "\n",
        "\n",
        "    G = Model(inputs=nin, outputs=nn, name=\"generator\")\n",
        "    return G\n",
        "\n",
        "\n",
        "# discriminator\n",
        "def get_D(input_shape):\n",
        "\n",
        "    g_init= tf.random_normal_initializer(1., 0.02)\n",
        "    ly_relu= LeakyReLU(alpha= 0.2)\n",
        "    df_dim = 16\n",
        "\n",
        "    nin = Input(input_shape)\n",
        "    n = Conv2D(64, (4, 4), (2, 2), padding='SAME', kernel_initializer='HeNormal')(nin)\n",
        "    n= ly_relu(n)\n",
        "\n",
        "    for i in range(2, 6):\n",
        "        n = Conv2D(df_dim*(2**i),(4, 4), (2, 2), padding='SAME', kernel_initializer='HeNormal')(n)\n",
        "        n= ly_relu(n)\n",
        "        n= BatchNormalization(gamma_initializer= g_init)(n)\n",
        "\n",
        "    n= Conv2D(df_dim*16, (1, 1), (1, 1), padding='SAME', kernel_initializer='HeNormal')(n)\n",
        "    n= ly_relu(n)\n",
        "    n= BatchNormalization(gamma_initializer= g_init)(n)\n",
        "\n",
        "    n= Conv2D(df_dim*8, (1, 1), (1, 1), padding='SAME', kernel_initializer='HeNormal')(n)\n",
        "    n= BatchNormalization(gamma_initializer= g_init)(n)\n",
        "    temp= n\n",
        "\n",
        "    n= Conv2D(df_dim*4, (3, 3), (1, 1), padding='SAME', kernel_initializer='HeNormal')(n)\n",
        "    n= ly_relu(n)\n",
        "    n= BatchNormalization(gamma_initializer= g_init)(n)\n",
        "\n",
        "    n= Conv2D(df_dim*8, (3, 3), (1, 1), padding='SAME', kernel_initializer='HeNormal')(n)\n",
        "    n= BatchNormalization(gamma_initializer= g_init)(n)\n",
        "\n",
        "    n= add([n, temp])\n",
        "\n",
        "    n= Flatten()(n)\n",
        "    no= Dense(units=1, kernel_initializer='HeNormal', activation= 'sigmoid')(n)\n",
        "    D= Model(inputs=nin, outputs=no, name=\"discriminator\")\n",
        "\n",
        "    return D\n",
        "\n",
        "\n",
        "# VGG19\n",
        "def get_vgg19():\n",
        "    vgg= tf.keras.applications.VGG19( include_top=False, weights='imagenet', \n",
        "                                    input_tensor=None, input_shape=(384, 384, 3),\n",
        "                                    pooling=None, classes=1000, classifier_activation='softmax' )\n",
        "\n",
        "    inp= Input(shape=(384, 384, 3))\n",
        "    x= vgg.layers[0](inp)\n",
        "    for ly in vgg.layers[1:17]:\n",
        "        x= ly(x)\n",
        "    VGG19= Model(inp, x)\n",
        "\n",
        "    return VGG19"
      ],
      "metadata": {
        "trusted": true,
        "id": "ajsx2Mq0ti5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Model creation"
      ],
      "metadata": {
        "id": "TUxp6FHTti5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "G = get_G((96, 96, 3))\n",
        "D = get_D((384, 384, 3))\n",
        "vgg= get_vgg19()"
      ],
      "metadata": {
        "trusted": true,
        "id": "z1syTyYNti5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizers\n",
        "g_optimizer_init = tf.optimizers.Adam(lr_v)\n",
        "g_optimizer = tf.optimizers.Adam(lr_v)\n",
        "d_optimizer = tf.optimizers.Adam(lr_v)"
      ],
      "metadata": {
        "trusted": true,
        "id": "zTOIpkixti5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize learning (G)"
      ],
      "metadata": {
        "id": "7kUygQXcti5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_step_epoch = round(n_epoch_init // batch_size)\n",
        "for epoch in range(n_epoch_init):\n",
        "  i,j= ((epoch)*batch_size)%tot_sample, (((epoch+1))*batch_size)%tot_sample\n",
        "  if j== 0:\n",
        "    j= -1\n",
        "  X, Y= LR_train[i: j], HR_train[i: j]\n",
        "  with tf.GradientTape() as tape:\n",
        "      ypred= G(X)\n",
        "      mse_loss= tf.reduce_mean(tf.reduce_mean(tf.math.squared_difference(Y, ypred), axis=-1))\n",
        "      grad = tape.gradient(mse_loss, G.trainable_weights)\n",
        "      g_optimizer_init.apply_gradients(zip(grad, G.trainable_weights))\n",
        "        \n",
        "  print(\"Epoch: [{}/{}] step: mse: {:.3f} \".format(\n",
        "            epoch, n_epoch_init , mse_loss))\n",
        "  if epoch%10 ==0:\n",
        "    img= G.predict(LR_train[np.newaxis, save_ind])[0]\n",
        "    #img= (img-img.mean())/img.std()\n",
        "    img= Image.fromarray(np.uint8(img*255))\n",
        "    img.save(os.path.join(save_dir, 'init_g_{}.png'.format(epoch)))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "V89wWFrPti5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f, ax= plt.subplots(1,5, figsize=(16, 6))\n",
        "for i, file in enumerate(glob.glob('./samples/init*')):\n",
        "    img= load(file, shape=(384, 384))\n",
        "    ax[i].imshow(img)\n",
        "plt.show()    "
      ],
      "metadata": {
        "trusted": true,
        "id": "ZQSHGoSHti5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### G.+ D. learning"
      ],
      "metadata": {
        "id": "5ktJqZVqti5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epoch= n_epoch-200\n",
        "for epoch in range(n_epoch):\n",
        "        i,j= ((epoch)*batch_size)%tot_sample, (((epoch+1))*batch_size)%tot_sample\n",
        "        if j== 0:\n",
        "            j= -1\n",
        "        X, Y= LR_train[i: j], HR_train[i: j]\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            fake_img= G(X)\n",
        "            fake_logits= D(fake_img)\n",
        "            real_logits= D(Y)\n",
        "            fake_feature= vgg(fake_img)\n",
        "            real_feature= vgg(Y)\n",
        "\n",
        "            #D. loss\n",
        "            d_loss1= tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(fake_logits , tf.zeros_like(fake_logits)))\n",
        "            d_loss2= tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(real_logits,tf.ones_like(real_logits)))\n",
        "            d_loss= d_loss1 + d_loss2\n",
        "\n",
        "            #G. loss\n",
        "            g_gan_loss= 2e-3*tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(fake_logits , tf.ones_like(fake_logits)))\n",
        "            mse_loss=  2e-1* tf.reduce_mean(tf.reduce_mean(tf.math.squared_difference(fake_img, Y), axis=-1))\n",
        "            vgg_loss = 2e-6 * tf.reduce_mean(tf.reduce_mean(tf.math.squared_difference(fake_feature, real_feature), axis=-1))\n",
        "            g_loss = mse_loss + vgg_loss + g_gan_loss\n",
        "\n",
        "            grad = tape.gradient(g_loss, G.trainable_weights)\n",
        "            g_optimizer.apply_gradients(zip(grad, G.trainable_weights))\n",
        "            grad = tape.gradient(d_loss, D.trainable_weights)\n",
        "            d_optimizer.apply_gradients(zip(grad, D.trainable_weights))\n",
        "\n",
        "        print(\"Epoch: [{}/{}] step: D.loss: {:.3f}: G.loss: {:.3f}\".format(\n",
        "                epoch, n_epoch , d_loss, g_loss))\n",
        "\n",
        "\n",
        "        if epoch%20 ==0:\n",
        "            img= G.predict(LR_train[np.newaxis, save_ind])[0]\n",
        "            # if not sigmoid\n",
        "            #img= (img-img.mean())/img.std()\n",
        "            img= Image.fromarray(np.uint8(img*255))\n",
        "            img.save(os.path.join(save_dir, 'train_g_{}.png'.format(epoch)))"
      ],
      "metadata": {
        "trusted": true,
        "id": "NLUBuPJUti5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f, ax= plt.subplots(3,5, figsize=(14, 16))\n",
        "for i, file in enumerate(glob.glob('./samples/train*')[:15]):\n",
        "    img= load(file, shape=(384, 384))\n",
        "    ax[i//5][i%5].imshow(img, aspect='auto')\n",
        "plt.show()    "
      ],
      "metadata": {
        "trusted": true,
        "id": "3n1x8Tp6ti5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results"
      ],
      "metadata": {
        "id": "oqdJlhqyti5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f, ax= plt.subplots(1,3, figsize=(16, 6))\n",
        "ax[0].imshow(LR_train[save_ind], aspect='auto')\n",
        "ax[1].imshow(load(glob.glob('./samples/train*')[-1], (384, 384)), aspect='auto')\n",
        "ax[2].imshow(HR_train[save_ind], aspect='auto')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "zSjfqqwNti5a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}